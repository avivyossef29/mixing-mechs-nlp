{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avivyossef29/mixing-mechs-nlp/blob/nitzan%2Fbinding-tests/XGLM_7_5b_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install core libs (No IPython upgrade)\n",
        "!pip install -q bitsandbytes transformers accelerate ipywidgets peft\n",
        "!pip install -q \"protobuf<6.0.0\" \"tensorboard<2.20.0\" sentencepiece tqdm tabulate\n",
        "\n",
        "# Re-clone your repo\n",
        "!git clone https://github.com/NitzanZacharia/mixing-mechs-nlp.git"
      ],
      "metadata": {
        "id": "m4xV449WDig6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import importlib\n",
        "from types import ModuleType\n",
        "\n",
        "# This \"tricks\" Python into thinking 'imp' still exists\n",
        "if 'imp' not in sys.modules:\n",
        "    imp = ModuleType('imp')\n",
        "    imp.reload = importlib.reload\n",
        "    sys.modules['imp'] = imp\n",
        "    print(\"âœ… 'imp' module shimmed successfully. Autoreload should now work.\")"
      ],
      "metadata": {
        "id": "Dt58hDfxDr26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyvene"
      ],
      "metadata": {
        "id": "84w_FqtYEWIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nnsight"
      ],
      "metadata": {
        "id": "FGmY8TqkFH0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U traitlets"
      ],
      "metadata": {
        "id": "V-1Yr9u5FOTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup (run once)\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Move to repo and add to path\n",
        "repo_dir = '/content/mixing-mechs-nlp'\n",
        "os.chdir(repo_dir)\n",
        "sys.path.append(repo_dir)\n",
        "sys.path.append(os.path.join(repo_dir, \"CausalAbstraction\"))\n",
        "\n",
        "# Your logging and imports\n",
        "import logging\n",
        "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.ERROR)\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from grammar.schemas import SCHEMA_BOXES\n",
        "from grammar.task_to_causal_model import multi_order_multi_schema_task_to_lookbacks_generic_causal_model\n",
        "from training import get_counterfactual_datasets, sample_answerable_question_template\n",
        "\n",
        "print(\"ðŸš€ Script initialized successfully!\")"
      ],
      "metadata": {
        "id": "f3mllH4QDxLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "model_id = \"facebook/xglm-7.5B\"\n",
        "schema = SCHEMA_BOXES\n",
        "num_instances = 20  # Number of binding groups (n=20 as in paper)\n",
        "num_samples = 100   # Number of test samples (use 3000 for full test)\n",
        "cat_indices_to_query = [0]  # Query by Object\n",
        "cat_to_query = 1            # Answer is Box\n"
      ],
      "metadata": {
        "id": "vViaG8-CFh1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5Ti0sWbpbwzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "LjBCA9XJGQ-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# 1. Configure 4-bit quantization (Essential for T4)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16  # GPU likes float16, not float32\n",
        ")\n",
        "\n",
        "print(f\"[+] Loading model: {model_id} on GPU...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",              # \"auto\" tells it to use the GPU (CUDA:0)\n",
        "    low_cpu_mem_usage=True,         # Prevents crashing system RAM\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "\n",
        "print(f\"âœ… Model loaded on: {model.device}\")\n"
      ],
      "metadata": {
        "id": "TlcmWtYqGKvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"    Model has {model.config.num_layers} layers\")"
      ],
      "metadata": {
        "id": "sEC7o8v2JRBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Test Dataset\n",
        "print(f\"[+] Generating test dataset with {num_instances} binding groups...\")\n",
        "causal_model = multi_order_multi_schema_task_to_lookbacks_generic_causal_model(\n",
        "    [schema], num_instances, num_fillers_per_item=0, fillers=False\n",
        ")\n",
        "causal_models = {schema.name: causal_model}\n",
        "\n",
        "train_ds, test_ds, _ = get_counterfactual_datasets(\n",
        "    None,  # No pipeline for filtering\n",
        "    [schema],\n",
        "    num_samples=num_samples,\n",
        "    num_instances=num_instances,\n",
        "    cat_indices_to_query=cat_indices_to_query,\n",
        "    answer_cat_id=cat_to_query,\n",
        "    do_assert=False,\n",
        "    do_filter=False,\n",
        "    causal_models=causal_models,\n",
        "    sample_an_answerable_question=sample_answerable_question_template,\n",
        ")\n",
        "\n",
        "train = train_ds[schema.name][schema.name]\n",
        "print(f\"    Generated {len(train)} samples\")\n"
      ],
      "metadata": {
        "id": "S718gxjnJYlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def format_xglm_binding_prompt(prompt: str) -> str:\n",
        "    # Extract the core logical statement\n",
        "    context = prompt.split(' Respond')[0]\n",
        "    try:\n",
        "        question_part = prompt.split(\"nothing else:\")[1].split(\"Box Answer:\")[0].strip()\n",
        "    except IndexError:\n",
        "        question_part = \"Which box is the object in?\"\n",
        "\n",
        "    # Few-Shot with explicit Language Tags (en:)\n",
        "    # This keeps XGLM grounded in English\n",
        "    few_shot = (\n",
        "        \"en: The scarf is in Box X, the basketball is in Box Y. \"\n",
        "        \"Question: Which box is the scarf in? Answer: Box X\\n\"\n",
        "        \"###\\n\"\n",
        "        \"en: The cookie is in Box J, the keyboard is in Box K. \"\n",
        "        \"Question: Which box is the keyboard in? Answer: Box K\\n\"\n",
        "        \"###\\n\"\n",
        "    )\n",
        "\n",
        "    return f\"{few_shot}en: {context} Question: {question_part} Answer: Box\"\n",
        "\n",
        "def naive_box_checker(response: str, expected: str) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if the expected letter appears as a standalone word.\n",
        "    Does NOT handle the 'BoxH' (no space) edge case.\n",
        "    \"\"\"\n",
        "    # Find all words (sequences of alphanumeric characters)\n",
        "    words = re.findall(r'\\w+', response.upper())\n",
        "\n",
        "    # Check if the expected letter is in that list\n",
        "    return expected.strip().upper() in words\n",
        "\n",
        "def generate_response(text, max_new_tokens=5):\n",
        "    # Always add the BOS token for XGLM\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
        "\n",
        "    # XGLM often generates \"</s>\" (token 2) to end a sentence\n",
        "    stop_strings = [\"\\n\", \"###\", \"</s>\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    decoded_output = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    # Clean up output\n",
        "    clean_output = decoded_output\n",
        "    for stop_word in stop_strings:\n",
        "        if stop_word in clean_output:\n",
        "            clean_output = clean_output.split(stop_word)[0]\n",
        "\n",
        "    return clean_output.strip()"
      ],
      "metadata": {
        "id": "wdCiTI40Jgdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#73 so far\n",
        "def format_xglm_binding_prompt3(raw_context: str, question_object: str) -> str:\n",
        "    # 1. Start with the BOS token\n",
        "    bos = \"<s>\"\n",
        "\n",
        "    # 2. Stronger Few-Shot (Using 3-4 objects helps the model handle longer lists)\n",
        "    few_shot = (\n",
        "        \"The ocarina is in Box V, the abacus is in Box Q, the flute is in Box L. \"\n",
        "        \"Question: Which box is the ocarina in? Answer: Box V. ### \"\n",
        "        \"The sextant is in Box X, the plectrum is in Box Z, the anchor is in Box M. \"\n",
        "        \"Question: Which box is the sextant in? Answer: Box X. ### \"\n",
        "    )\n",
        "\n",
        "    # 3. Clean the raw_context\n",
        "    # This removes the \"Respond in one word...\" part that is already inside the data\n",
        "    # It keeps only the sentences describing where objects are.\n",
        "    clean_context = raw_context.split(\"Respond in one word\")[0].strip()\n",
        "\n",
        "    # 4. Final Formatting\n",
        "    # We ensure a space after the context and a clean question at the end.\n",
        "    test_context = f\"{clean_context} \"\n",
        "    question = f\"Question: Which box is the {question_object} in? Answer: Box\"\n",
        "\n",
        "    return f\"{bos}{few_shot}{test_context}{question}\"\n",
        ""
      ],
      "metadata": {
        "id": "ocQE0Ltnsgxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#also 73\n",
        "def format_xglm_binding_prompt2(raw_context: str, question_object: str) -> str:\n",
        "    # 1. REMOVE MANUAL BOS (The tokenizer handles this)\n",
        "    # Removing the manual \"<s>\" prevents the double-start-token confusion.\n",
        "\n",
        "    # 2. DIVERSE FEW-SHOTS (Small Adjustment)\n",
        "    # Example 1: Queries the FIRST item (Ocarina) -> Teaches Primacy\n",
        "    # Example 2: Queries the LAST item (Anchor) -> Teaches Recency\n",
        "    # This balance helps the model cover the whole context window.\n",
        "    few_shot = (\n",
        "        \"The ocarina is in Box V, the abacus is in Box Q, the flute is in Box L. \"\n",
        "        \"Question: Which box is the ocarina in? Answer: Box V. ### \"\n",
        "\n",
        "        \"The sextant is in Box X, the plectrum is in Box Z, the anchor is in Box M. \"\n",
        "        \"Question: Which box is the plectrum in? Answer: Box Z. ### \"\n",
        "    )\n",
        "\n",
        "    # 3. Clean Context (Kept exactly as you had it)\n",
        "    clean_context = raw_context.split(\"Respond in one word\")[0].strip()\n",
        "\n",
        "    # 4. Final Formatting\n",
        "    # We add a newline before the Question to help separation.\n",
        "    test_context = f\"{clean_context} \"\n",
        "    question = f\"Question: Which box is the {question_object} in? Answer: Box\"\n",
        "\n",
        "    return f\"{few_shot}{test_context}{question}\"\n",
        "\n",
        "# Keep your generation function exactly the same\n",
        "def generate_response(text, max_new_tokens=2):\n",
        "    # Ensure add_special_tokens=True is ON (default)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
        "\n",
        "    stop_strings = [\"\\n\", \"###\", \"</s>\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    decoded_output = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    clean_output = decoded_output\n",
        "    for stop_word in stop_strings:\n",
        "        if stop_word in clean_output:\n",
        "            clean_output = clean_output.split(stop_word)[0]\n",
        "\n",
        "    return clean_output.strip()"
      ],
      "metadata": {
        "id": "Mml_0cJj5iGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Single Sample (for debugging)\n",
        "sample = train[0]\n",
        "raw_input = sample[\"input\"][\"raw_input\"]\n",
        "queried_object = sample[\"input\"].get(\"Object.0.Query\", None)\n",
        "\n",
        "print(\"Raw input\")\n",
        "print(raw_input)\n",
        "print(f\"\\nQueried object: {queried_object}\")\n",
        "\n",
        "# Get expected answer\n",
        "forward_res = causal_model.run_forward(sample[\"input\"])\n",
        "expected = forward_res.get(\"answer\", \"\")\n",
        "if isinstance(expected, dict):\n",
        "    expected = max(expected, key=lambda k: expected[k])\n",
        "print(f\"Expected answer: {expected}\")\n",
        "\n",
        "# Format and generate\n",
        "prompt = format_xglm_binding_prompt2(raw_input, queried_object)\n",
        "print(f\"\\nFormatted prompt\")\n",
        "print(prompt)\n",
        "\n",
        "response = generate_response(prompt)\n",
        "print(f\"\\nModel response: '{response}'\")\n",
        "print(f\"Correct (strict): {naive_box_checker(response, expected)}\")\n",
        "print(f\"Correct (original): {schema.checker(response, expected)}\")\n"
      ],
      "metadata": {
        "id": "U4HSY99qJjUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# Run Full Evaluation\n",
        "print(f\"[+] Evaluating {len(train)} samples...\")\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "position_correct = {}\n",
        "position_total = {}\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(len(train))):\n",
        "    sample = train[i]\n",
        "    raw_input = sample[\"input\"][\"raw_input\"]\n",
        "    queried_object = sample[\"input\"].get(\"Object.0.Query\", None)\n",
        "\n",
        "    # Get expected answer\n",
        "    forward_res = causal_model.run_forward(sample[\"input\"])\n",
        "    expected = forward_res.get(\"answer\", \"\")\n",
        "    if isinstance(expected, dict):\n",
        "        expected = max(expected, key=lambda k: expected[k])\n",
        "\n",
        "    # Get query position\n",
        "    query_position = sample[\"input\"].get(\"metadata\", {}).get(\"src_positional_index\", -1)\n",
        "\n",
        "    # Format and generate\n",
        "    prompt = format_xglm_binding_prompt2(raw_input, queried_object)\n",
        "    response = generate_response(prompt)\n",
        "\n",
        "    # Check correctness using STRICT checker\n",
        "    is_correct = naive_box_checker(response, expected)\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "    # Track per-position accuracy\n",
        "    if query_position >= 0:\n",
        "        if query_position not in position_correct:\n",
        "            position_correct[query_position] = 0\n",
        "            position_total[query_position] = 0\n",
        "        if is_correct:\n",
        "            position_correct[query_position] += 1\n",
        "        position_total[query_position] += 1\n",
        "\n",
        "    results.append({\n",
        "        \"expected\": expected,\n",
        "        \"response\": response,\n",
        "        \"correct\": is_correct,\n",
        "        \"position\": query_position,\n",
        "        \"queried_object\": queried_object,\n",
        "        \"full_query\": prompt,\n",
        "    })\n",
        "\n",
        "# Print results\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"RESULTS: XGLM-7.5B on Binding Task (n={num_instances})\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Overall Accuracy: {correct/total:.2%} ({correct}/{total})\")\n",
        "\n",
        "if position_total:\n",
        "    print(f\"\\nPer-Position Accuracy:\")\n",
        "    for pos in sorted(position_total.keys()):\n",
        "        pos_acc = position_correct[pos] / position_total[pos]\n",
        "        print(f\"  Position {pos}: {pos_acc:.2%} ({position_correct[pos]}/{position_total[pos]})\")\n"
      ],
      "metadata": {
        "id": "RBK1wg8SLI89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Filter for only incorrect responses\n",
        "correct_ans = df[df['correct'] == True]\n",
        "\n",
        "print(f\"Total successes: {len(correct_ans)}\")\n",
        "print(\"\\n--- Examples of correct Answers ---\")\n",
        "# Display the first 20 failures\n",
        "# We use .head(20) to see enough variety\n",
        "display(correct_ans[['queried_object', 'expected', 'response', 'position']].head(20))\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "failed = df[df['correct'] == False]\n",
        "\n",
        "print(f\"Total Failures: {len(failed)}\")\n",
        "print(\"\\n--- Examples of incorrect Answers ---\")\n",
        "# Display the first 20 failures\n",
        "# We use .head(20) to see enough variety\n",
        "display(failed[['queried_object', 'expected', 'response', 'position','full_query']].head(20))"
      ],
      "metadata": {
        "id": "vZI5neWLNJoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "hA3ZzS_Oldtl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}