{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binding Task Test - Multi-Model Support\n",
    "\n",
    "Test language models on the binding task with n=20 binding groups.\n",
    "Supports multiple models: MPT-7b-instruct, Falcon3-Mamba-7B-Base, and mamba-2.8b-instruct-openhermes.\n",
    "\n",
    "This notebook allows you to keep the model loaded between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup (run once)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"CausalAbstraction\")\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.ERROR)\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "\n",
    "from grammar.schemas import SCHEMA_BOXES\n",
    "from grammar.task_to_causal_model import multi_order_multi_schema_task_to_lookbacks_generic_causal_model\n",
    "from training import get_counterfactual_datasets, sample_answerable_question_template\n",
    "\n",
    "# Import model wrappers\n",
    "from binding_model_wrappers import create_model_wrapper\n",
    "\n",
    "def naive_checker(response: str, expected: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if `expected` appears as a standalone word in `response`,\n",
    "    ignoring case.\n",
    "    \"\"\"\n",
    "    pattern = rf'\\b{re.escape(expected)}\\b'\n",
    "    return re.search(pattern, response, flags=re.IGNORECASE) is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_id = \"Zyphra/Zamba2-2.7B-instruct\"\n",
    "# model_id = \"Zyphra/Zamba2-7B-instruct\"\n",
    "# model_id = \"state-spaces/mamba-2.8b-hf\"\n",
    "# model_id = \"mistralai/Mamba-Codestral-7B-v0.1\"\n",
    "# model_id = \"tiiuae/Falcon3-Mamba-7B-Instruct\"\n",
    "# model_id = \"state-spaces/mamba2-2.7b-hf\" // Not working.\n",
    "# model_id = \"clibrain/mamba-2.8b-instruct-openhermes\" // Not working.\n",
    "# model_id = \"mosaicml/mpt-7b-instruct\" // Recently removed from huggingface.\n",
    "\n",
    "schema = SCHEMA_BOXES\n",
    "num_instances = 20  # Number of binding groups (n=20 as in paper)\n",
    "num_samples = 10   # Number of test samples (use 3000 for full test)\n",
    "cat_indices_to_query = [0]  # Query by Object\n",
    "cat_to_query = 1            # Answer is Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model (run once - takes a few minutes)\n",
    "print(f\"[+] Loading model: {model_id}\")\n",
    "model_wrapper = create_model_wrapper(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"    Model loaded on {model_wrapper.model.device}\")\n",
    "print(f\"    Model has {model_wrapper.get_num_layers()} layers\")\n",
    "print(f\"    Model type: {model_wrapper.get_display_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Test Dataset\n",
    "print(f\"[+] Generating test dataset with {num_instances} binding groups...\")\n",
    "causal_model = multi_order_multi_schema_task_to_lookbacks_generic_causal_model(\n",
    "    [schema], num_instances, num_fillers_per_item=0, fillers=False\n",
    ")\n",
    "causal_models = {schema.name: causal_model}\n",
    "\n",
    "train_ds, test_ds, _ = get_counterfactual_datasets(\n",
    "    None,  # No pipeline for filtering\n",
    "    [schema],\n",
    "    num_samples=num_samples,\n",
    "    num_instances=num_instances,\n",
    "    cat_indices_to_query=cat_indices_to_query,\n",
    "    answer_cat_id=cat_to_query,\n",
    "    do_assert=False,\n",
    "    do_filter=False,\n",
    "    causal_models=causal_models,\n",
    "    sample_an_answerable_question=sample_answerable_question_template,\n",
    ")\n",
    "\n",
    "train = train_ds[schema.name][schema.name]\n",
    "print(f\"    Generated {len(train)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Single Sample (for debugging)\n",
    "sample = train[0]\n",
    "raw_input = sample[\"input\"][\"raw_input\"]\n",
    "queried_object = sample[\"input\"].get(\"Object.0.Query\", None)\n",
    "\n",
    "print(\"Raw input\")\n",
    "print(raw_input)\n",
    "print(f\"\\nQueried object: {queried_object}\")\n",
    "\n",
    "# Get expected answer\n",
    "forward_res = causal_model.run_forward(sample[\"input\"])\n",
    "expected = forward_res.get(\"answer\", \"\")\n",
    "if isinstance(expected, dict):\n",
    "    expected = max(expected, key=lambda k: expected[k])\n",
    "print(f\"Expected answer: {expected}\")\n",
    "\n",
    "# Format and generate using model wrapper\n",
    "prompt = model_wrapper.format_prompt(raw_input, queried_object=queried_object)\n",
    "print(f\"\\nFormatted prompt\")\n",
    "print(prompt)\n",
    "\n",
    "response = model_wrapper.generate_response(prompt)\n",
    "print(f\"\\nModel response: '{response}'\")\n",
    "print(f\"Correct (Naive): {naive_checker(response, expected)}\")\n",
    "# print(f\"Correct (original): {schema.checker(response, expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Full Evaluation\n",
    "print(f\"[+] Evaluating {len(train)} samples...\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "position_correct = {}\n",
    "position_total = {}\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(len(train))):\n",
    "    sample = train[i]\n",
    "    raw_input = sample[\"input\"][\"raw_input\"]\n",
    "    queried_object = sample[\"input\"].get(\"Object.0.Query\", None)\n",
    "    \n",
    "    # Get expected answer\n",
    "    forward_res = causal_model.run_forward(sample[\"input\"])\n",
    "    expected = forward_res.get(\"answer\", \"\")\n",
    "    if isinstance(expected, dict):\n",
    "        expected = max(expected, key=lambda k: expected[k])\n",
    "    \n",
    "    # Get query position\n",
    "    query_position = sample[\"input\"].get(\"metadata\", {}).get(\"src_positional_index\", -1)\n",
    "    \n",
    "    # Format and generate using model wrapper\n",
    "    prompt = model_wrapper.format_prompt(raw_input, queried_object=queried_object)\n",
    "    response = model_wrapper.generate_response(prompt)\n",
    "    \n",
    "    # Check correctness using naive checker\n",
    "    is_correct = naive_checker(response, expected)\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "    print(f\"{i=} {response=} {expected=} {is_correct=}\")\n",
    "    \n",
    "    # Track per-position accuracy\n",
    "    if query_position >= 0:\n",
    "        if query_position not in position_correct:\n",
    "            position_correct[query_position] = 0\n",
    "            position_total[query_position] = 0\n",
    "        if is_correct:\n",
    "            position_correct[query_position] += 1\n",
    "        position_total[query_position] += 1\n",
    "    \n",
    "    results.append({\n",
    "        \"expected\": expected,\n",
    "        \"response\": response,\n",
    "        \"correct\": is_correct,\n",
    "        \"position\": query_position,\n",
    "        \"queried_object\": queried_object,\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RESULTS: {model_wrapper.get_display_name()} on Binding Task (n={num_instances})\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Overall Accuracy: {correct/total:.2%} ({correct}/{total})\")\n",
    "\n",
    "if position_total:\n",
    "    print(f\"\\nPer-Position Accuracy:\")\n",
    "    for pos in sorted(position_total.keys()):\n",
    "        pos_acc = position_correct[pos] / position_total[pos]\n",
    "        print(f\"  Position {pos}: {pos_acc:.2%} ({position_correct[pos]}/{position_total[pos]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Specific Test Cells\n",
    "\n",
    "The cells below allow you to quickly test each supported model with a single sample.\n",
    "Change the model_id in the configuration cell above and run the corresponding test cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Errors\n",
    "print(\"Sample of incorrect predictions:\")\n",
    "incorrect = [r for r in results if not r[\"correct\"]]\n",
    "correct = [r for r in results if r[\"correct\"]]\n",
    "for r in incorrect[:10]:\n",
    "    print(f\"  Object: {r['queried_object']}, Expected: {r['expected']}, Got: '{r['response']}', Position: {r['position']}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "for r in correct[:10]:\n",
    "    print(f\"  Object: {r['queried_object']}, Expected: {r['expected']}, Got: '{r['response']}', Position: {r['position']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
