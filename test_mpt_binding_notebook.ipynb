{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPT-7b ALiBi Binding Task Test\n",
    "\n",
    "Test the mosaicml/mpt-7b-instruct model on the binding task with n=20 binding groups.\n",
    "This notebook allows you to keep the model loaded between runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n"
     ]
    }
   ],
   "source": [
    "# Setup (run once)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"CausalAbstraction\")\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.ERROR)\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from grammar.schemas import SCHEMA_BOXES\n",
    "from grammar.task_to_causal_model import multi_order_multi_schema_task_to_lookbacks_generic_causal_model\n",
    "from training import get_counterfactual_datasets, sample_answerable_question_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_id = \"mosaicml/mpt-7b-instruct\"\n",
    "schema = SCHEMA_BOXES\n",
    "num_instances = 20  # Number of binding groups (n=20 as in paper)\n",
    "num_samples = 100   # Number of test samples (use 3000 for full test)\n",
    "cat_indices_to_query = [0]  # Query by Object\n",
    "cat_to_query = 1            # Answer is Box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loading model: mosaicml/mpt-7b-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9452ab3c441d4ce78041b1826da82dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model loaded on cpu\n",
      "    Model has 32 layers\n"
     ]
    }
   ],
   "source": [
    "# Load Model (run once - takes a few minutes)\n",
    "print(f\"[+] Loading model: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",  # Avoid flash attention issues\n",
    ")\n",
    "print(f\"    Model loaded on {model.device}\")\n",
    "print(f\"    Model has {model.config.n_layers} layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Generating test dataset with 20 binding groups...\n",
      "    Generated 100 samples\n"
     ]
    }
   ],
   "source": [
    "# Generate Test Dataset\n",
    "print(f\"[+] Generating test dataset with {num_instances} binding groups...\")\n",
    "causal_model = multi_order_multi_schema_task_to_lookbacks_generic_causal_model(\n",
    "    [schema], num_instances, num_fillers_per_item=0, fillers=False\n",
    ")\n",
    "causal_models = {schema.name: causal_model}\n",
    "\n",
    "train_ds, test_ds, _ = get_counterfactual_datasets(\n",
    "    None,  # No pipeline for filtering\n",
    "    [schema],\n",
    "    num_samples=num_samples,\n",
    "    num_instances=num_instances,\n",
    "    cat_indices_to_query=cat_indices_to_query,\n",
    "    answer_cat_id=cat_to_query,\n",
    "    do_assert=False,\n",
    "    do_filter=False,\n",
    "    causal_models=causal_models,\n",
    "    sample_an_answerable_question=sample_answerable_question_template,\n",
    ")\n",
    "\n",
    "train = train_ds[schema.name][schema.name]\n",
    "print(f\"    Generated {len(train)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_mpt_instruct_prompt(prompt: str, queried_object: str = None) -> str:\n",
    "    # Clean up the raw input - remove the instruction and trailing \"Box Answer:\"\n",
    "    clean_prompt = prompt\n",
    "    \n",
    "    \n",
    "    return (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        f\"### Instruction:\\n{clean_prompt}\\n\\n\"\n",
    "        \"### Response:\\n \"  # <--- PRE-FILL with \"Box \" to force just the letter\n",
    "    )\n",
    "\n",
    "def naive_box_checker(response: str, expected: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the expected letter appears as a standalone word.\n",
    "    Does NOT handle the 'BoxH' (no space) edge case.\n",
    "    \"\"\"\n",
    "    # Find all words (sequences of alphanumeric characters)\n",
    "    words = re.findall(r'\\w+', response.upper())\n",
    "    \n",
    "    # Check if the expected letter is in that list\n",
    "    return expected.strip().upper() in words\n",
    "\n",
    "def generate_response(prompt, max_new_tokens=10):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw input\n",
      "The medicine is in Box Z, the ticket is in Box R, the chemical is in Box N, the plane is in Box D, the boat is in Box F, the block is in Box T, the drink is in Box S, the wheel is in Box O, the television is in Box U, the dress is in Box K, the train is in Box C, the crown is in Box H, the map is in Box E, the cross is in Box Q, the camera is in Box I, the branch is in Box X, the bread is in Box J, the shell is in Box M, the letter is in Box P, and the fig is in Box G. Respond in one word, only the answer and nothing else: Which box is the crown in? Box Answer:\n",
      "\n",
      "Queried object: crown\n",
      "Expected answer: H\n",
      "\n",
      "Formatted prompt (last 500 chars):\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "The medicine is in Box Z, the ticket is in Box R, the chemical is in Box N, the plane is in Box D, the boat is in Box F, the block is in Box T, the drink is in Box S, the wheel is in Box O, the television is in Box U, the dress is in Box K, the train is in Box C, the crown is in Box H, the map is in Box E, the cross is in Box Q, the camera is in Box I, the branch is in Box X, the bread is in Box J, the shell is in Box M, the letter is in Box P, and the fig is in Box G. Respond in one word, only the answer and nothing else: Which box is the crown in? Box Answer:\n",
      "\n",
      "### Response:\n",
      " \n",
      "\n",
      "Model response: '\n",
      "The crown is in Box H.'\n",
      "Correct (strict): True\n",
      "Correct (original): False\n"
     ]
    }
   ],
   "source": [
    "# Test Single Sample (for debugging)\n",
    "sample = train[0]\n",
    "raw_input = sample[\"input\"][\"raw_input\"]\n",
    "queried_object = sample[\"input\"].get(\"Object.0.Query\", None)\n",
    "\n",
    "print(\"Raw input\")\n",
    "print(raw_input)\n",
    "print(f\"\\nQueried object: {queried_object}\")\n",
    "\n",
    "# Get expected answer\n",
    "forward_res = causal_model.run_forward(sample[\"input\"])\n",
    "expected = forward_res.get(\"answer\", \"\")\n",
    "if isinstance(expected, dict):\n",
    "    expected = max(expected, key=lambda k: expected[k])\n",
    "print(f\"Expected answer: {expected}\")\n",
    "\n",
    "# Format and generate\n",
    "prompt = format_mpt_instruct_prompt(raw_input, queried_object=queried_object)\n",
    "print(f\"\\nFormatted prompt\")\n",
    "print(prompt)\n",
    "\n",
    "response = generate_response(prompt)\n",
    "print(f\"\\nModel response: '{response}'\")\n",
    "print(f\"Correct (strict): {naive_box_checker(response, expected)}\")\n",
    "print(f\"Correct (original): {schema.checker(response, expected)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluating 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5d1087b5874cea8f7a75b3018eebe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RESULTS: MPT-7b-instruct on Binding Task (n=20)\n",
      "==================================================\n",
      "Overall Accuracy: 100.00% (10/10)\n",
      "\n",
      "Per-Position Accuracy:\n",
      "  Position 1: 100.00% (1/1)\n",
      "  Position 3: 100.00% (1/1)\n",
      "  Position 7: 100.00% (1/1)\n",
      "  Position 8: 100.00% (1/1)\n",
      "  Position 10: 100.00% (1/1)\n",
      "  Position 11: 100.00% (3/3)\n",
      "  Position 18: 100.00% (1/1)\n",
      "  Position 19: 100.00% (1/1)\n"
     ]
    }
   ],
   "source": [
    "# Run Full Evaluation\n",
    "print(f\"[+] Evaluating {len(train)} samples...\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "position_correct = {}\n",
    "position_total = {}\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(len(train))):\n",
    "    sample = train[i]\n",
    "    raw_input = sample[\"input\"][\"raw_input\"]\n",
    "    queried_object = sample[\"input\"].get(\"Object.0.Query\", None)\n",
    "    \n",
    "    # Get expected answer\n",
    "    forward_res = causal_model.run_forward(sample[\"input\"])\n",
    "    expected = forward_res.get(\"answer\", \"\")\n",
    "    if isinstance(expected, dict):\n",
    "        expected = max(expected, key=lambda k: expected[k])\n",
    "    \n",
    "    # Get query position\n",
    "    query_position = sample[\"input\"].get(\"metadata\", {}).get(\"src_positional_index\", -1)\n",
    "    \n",
    "    # Format and generate\n",
    "    prompt = format_mpt_instruct_prompt(raw_input, queried_object=queried_object)\n",
    "    response = generate_response(prompt)\n",
    "    \n",
    "    # Check correctness using STRICT checker\n",
    "    is_correct = naive_box_checker(response, expected)\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "    # Track per-position accuracy\n",
    "    if query_position >= 0:\n",
    "        if query_position not in position_correct:\n",
    "            position_correct[query_position] = 0\n",
    "            position_total[query_position] = 0\n",
    "        if is_correct:\n",
    "            position_correct[query_position] += 1\n",
    "        position_total[query_position] += 1\n",
    "    \n",
    "    results.append({\n",
    "        \"expected\": expected,\n",
    "        \"response\": response,\n",
    "        \"correct\": is_correct,\n",
    "        \"position\": query_position,\n",
    "        \"queried_object\": queried_object,\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RESULTS: MPT-7b-instruct on Binding Task (n={num_instances})\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Overall Accuracy: {correct/total:.2%} ({correct}/{total})\")\n",
    "\n",
    "if position_total:\n",
    "    print(f\"\\nPer-Position Accuracy:\")\n",
    "    for pos in sorted(position_total.keys()):\n",
    "        pos_acc = position_correct[pos] / position_total[pos]\n",
    "        print(f\"  Position {pos}: {pos_acc:.2%} ({position_correct[pos]}/{position_total[pos]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of incorrect predictions:\n",
      "\n",
      "\n",
      "  Object: crown, Expected: H, Got: '\n",
      "The crown is in Box H.', Position: 11\n",
      "  Object: rock, Expected: Q, Got: '\n",
      "The rock is in Box Q.', Position: 11\n",
      "  Object: pot, Expected: A, Got: '\n",
      "The pot is in Box A.', Position: 10\n",
      "  Object: suit, Expected: L, Got: '\n",
      "The suit is in Box L.', Position: 8\n",
      "  Object: clock, Expected: K, Got: '\n",
      "The clock is in Box K.', Position: 1\n",
      "  Object: camera, Expected: E, Got: '\n",
      "The camera is in Box E.', Position: 7\n",
      "  Object: ticket, Expected: M, Got: '\n",
      "The ticket is in Box M.', Position: 19\n",
      "  Object: painting, Expected: M, Got: '\n",
      "The painting is in Box M.', Position: 18\n",
      "  Object: game, Expected: S, Got: '\n",
      "The game is in Box S.', Position: 11\n",
      "  Object: machine, Expected: K, Got: '\n",
      "The machine is in Box K.', Position: 3\n",
      "\n",
      "\n",
      "\n",
      "The crown is in Box H.\n",
      "\n",
      "The rock is in Box Q.\n",
      "\n",
      "The pot is in Box A.\n",
      "\n",
      "The suit is in Box L.\n",
      "\n",
      "The clock is in Box K.\n",
      "\n",
      "The camera is in Box E.\n",
      "\n",
      "The ticket is in Box M.\n",
      "\n",
      "The painting is in Box M.\n",
      "\n",
      "The game is in Box S.\n",
      "\n",
      "The machine is in Box K.\n"
     ]
    }
   ],
   "source": [
    "# Analyze Errors\n",
    "print(\"Sample of incorrect predictions:\")\n",
    "incorrect = [r for r in results if not r[\"correct\"]]\n",
    "correct = [r for r in results if r[\"correct\"]]\n",
    "for r in incorrect[:10]:\n",
    "    print(f\"  Object: {r['queried_object']}, Expected: {r['expected']}, Got: '{r['response']}', Position: {r['position']}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "for r in correct[:10]:\n",
    "    print(f\"  Object: {r['queried_object']}, Expected: {r['expected']}, Got: '{r['response']}', Position: {r['position']}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "for r in results[:10]:\n",
    "    print(f\"{r['response']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
