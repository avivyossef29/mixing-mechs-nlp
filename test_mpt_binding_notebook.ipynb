{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binding Task Test - Multi-Model Support\n",
    "\n",
    "Test language models on the binding task with n=20 binding groups.\n",
    "Supports multiple models: MPT-7b-instruct, Falcon3-Mamba-7B-Base, and mamba-2.8b-instruct-openhermes.\n",
    "\n",
    "This notebook allows you to keep the model loaded between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n"
     ]
    }
   ],
   "source": [
    "# Setup (run once)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"CausalAbstraction\")\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.ERROR)\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "\n",
    "from grammar.schemas import SCHEMA_BOXES\n",
    "from grammar.task_to_causal_model import multi_order_multi_schema_task_to_lookbacks_generic_causal_model\n",
    "from training import get_counterfactual_datasets, sample_answerable_question_template\n",
    "\n",
    "# Import model wrappers\n",
    "from binding_model_wrappers import create_model_wrapper\n",
    "\n",
    "def naive_checker(response: str, expected: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if `expected` appears as a standalone word in `response`,\n",
    "    ignoring case.\n",
    "    \"\"\"\n",
    "    pattern = rf'\\b{re.escape(expected)}\\b'\n",
    "    return re.search(pattern, response, flags=re.IGNORECASE) is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "# model_id = \"state-spaces/mamba-2.8b-hf\"\n",
    "# model_id = \"mistralai/Mamba-Codestral-7B-v0.1\"\n",
    "model_id = \"tiiuae/Falcon3-Mamba-7B-Instruct\"\n",
    "# model_id = \"clibrain/mamba-2.8b-instruct-openhermes\" // Not working.\n",
    "# model_id = \"mosaicml/mpt-7b-instruct\" // Recently removed from huggingface.\n",
    "\n",
    "schema = SCHEMA_BOXES\n",
    "num_instances = 20  # Number of binding groups (n=20 as in paper)\n",
    "num_samples = 10   # Number of test samples (use 3000 for full test)\n",
    "cat_indices_to_query = [0]  # Query by Object\n",
    "cat_to_query = 1            # Answer is Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loading model: tiiuae/Falcon3-Mamba-7B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9612584dc24e47d7a48a8977d35c2b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta params: 0\n",
      "    Model loaded on cpu\n",
      "    Model has 64 layers\n",
      "    Model type: Falcon3-Mamba-7B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Load Model (run once - takes a few minutes)\n",
    "print(f\"[+] Loading model: {model_id}\")\n",
    "model_wrapper = create_model_wrapper(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"    Model loaded on {model_wrapper.model.device}\")\n",
    "print(f\"    Model has {model_wrapper.get_num_layers()} layers\")\n",
    "print(f\"    Model type: {model_wrapper.get_display_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Generating test dataset with 20 binding groups...\n",
      "    Generated 10 samples\n"
     ]
    }
   ],
   "source": [
    "# Generate Test Dataset\n",
    "print(f\"[+] Generating test dataset with {num_instances} binding groups...\")\n",
    "causal_model = multi_order_multi_schema_task_to_lookbacks_generic_causal_model(\n",
    "    [schema], num_instances, num_fillers_per_item=0, fillers=False\n",
    ")\n",
    "causal_models = {schema.name: causal_model}\n",
    "\n",
    "train_ds, test_ds, _ = get_counterfactual_datasets(\n",
    "    None,  # No pipeline for filtering\n",
    "    [schema],\n",
    "    num_samples=num_samples,\n",
    "    num_instances=num_instances,\n",
    "    cat_indices_to_query=cat_indices_to_query,\n",
    "    answer_cat_id=cat_to_query,\n",
    "    do_assert=False,\n",
    "    do_filter=False,\n",
    "    causal_models=causal_models,\n",
    "    sample_an_answerable_question=sample_answerable_question_template,\n",
    ")\n",
    "\n",
    "train = train_ds[schema.name][schema.name]\n",
    "print(f\"    Generated {len(train)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw input\n",
      "The boat is in Box D, the pot is in Box R, the computer is in Box V, the file is in Box S, the tea is in Box T, the dress is in Box Q, the engine is in Box W, the wheel is in Box Z, the guitar is in Box G, the stone is in Box E, the leaf is in Box K, the ball is in Box F, the tie is in Box B, the train is in Box J, the cup is in Box H, the mirror is in Box M, the suit is in Box P, the letter is in Box O, the fan is in Box A, and the creature is in Box L. Respond in one word, only the answer and nothing else: Which box is the engine in? Box Answer:\n",
      "\n",
      "Queried object: engine\n",
      "Expected answer: W\n",
      "\n",
      "Formatted prompt\n",
      "<|begin_of_text|><|im_start|>system\n",
      "You are a helpful friendly assistant Falcon3 from TII, try to follow instructions as much as possible.<|im_end|>\n",
      "<|im_start|>user\n",
      "The boat is in Box D, the pot is in Box R, the computer is in Box V, the file is in Box S, the tea is in Box T, the dress is in Box Q, the engine is in Box W, the wheel is in Box Z, the guitar is in Box G, the stone is in Box E, the leaf is in Box K, the ball is in Box F, the tie is in Box B, the train is in Box J, the cup is in Box H, the mirror is in Box M, the suit is in Box P, the letter is in Box O, the fan is in Box A, and the creature is in Box L. Respond in one word, only the answer and nothing else: Which box is the engine in? Box Answer:<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "Model response: 'G'\n",
      "Correct (Naive): False\n"
     ]
    }
   ],
   "source": [
    "# Test Single Sample (for debugging)\n",
    "sample = train[0]\n",
    "raw_input = sample[\"input\"][\"raw_input\"]\n",
    "queried_object = sample[\"input\"].get(\"Object.0.Query\", None)\n",
    "\n",
    "print(\"Raw input\")\n",
    "print(raw_input)\n",
    "print(f\"\\nQueried object: {queried_object}\")\n",
    "\n",
    "# Get expected answer\n",
    "forward_res = causal_model.run_forward(sample[\"input\"])\n",
    "expected = forward_res.get(\"answer\", \"\")\n",
    "if isinstance(expected, dict):\n",
    "    expected = max(expected, key=lambda k: expected[k])\n",
    "print(f\"Expected answer: {expected}\")\n",
    "\n",
    "# Format and generate using model wrapper\n",
    "prompt = model_wrapper.format_prompt(raw_input, queried_object=queried_object)\n",
    "print(f\"\\nFormatted prompt\")\n",
    "print(prompt)\n",
    "\n",
    "response = model_wrapper.generate_response(prompt)\n",
    "print(f\"\\nModel response: '{response}'\")\n",
    "print(f\"Correct (Naive): {naive_checker(response, expected)}\")\n",
    "# print(f\"Correct (original): {schema.checker(response, expected)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluating 10 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2284090ee68f42578ddd3b924d2dc95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 response='G' expected='W' is_correct=False\n",
      "i=1 response='E' expected='F' is_correct=False\n",
      "i=2 response='Box A' expected='A' is_correct=True\n",
      "i=3 response='D' expected='D' is_correct=True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Format and generate using model wrapper\u001b[39;00m\n\u001b[32m     25\u001b[39m prompt = model_wrapper.format_prompt(raw_input, queried_object=queried_object)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m response = \u001b[43mmodel_wrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Check correctness using naive checker\u001b[39;00m\n\u001b[32m     29\u001b[39m is_correct = naive_checker(response, expected)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/mixing-mechs/binding_model_wrappers.py:213\u001b[39m, in \u001b[36mFalconMambaModelWrapper.generate_response\u001b[39m\u001b[34m(self, prompt, max_new_tokens)\u001b[39m\n\u001b[32m    210\u001b[39m inputs = {k: v.to(param_device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer.decode(outputs[\u001b[32m0\u001b[39m][inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m1\u001b[39m]:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/transformers/generation/utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/transformers/models/falcon_mamba/modeling_falcon_mamba.py:900\u001b[39m, in \u001b[36mFalconMambaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, cache_params, labels, output_hidden_states, return_dict, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    888\u001b[39m \u001b[33;03mcache_params (`FalconMambaCache`, *optional*):\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[33;03m    If passed along, the model uses the previous state in all the blocks (which will give the output for the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    896\u001b[39m \u001b[33;03m    If set to `True`, the `cache_params` is returned and can be used to quickly generate the next logits.\u001b[39;00m\n\u001b[32m    897\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    898\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m falcon_mamba_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m hidden_states = falcon_mamba_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    912\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(hidden_states.to(\u001b[38;5;28mself\u001b[39m.lm_head.weight.dtype)).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/transformers/models/falcon_mamba/modeling_falcon_mamba.py:759\u001b[39m, in \u001b[36mFalconMambaModel.forward\u001b[39m\u001b[34m(self, input_ids, inputs_embeds, cache_params, use_cache, output_hidden_states, return_dict, cache_position, attention_mask)\u001b[39m\n\u001b[32m    757\u001b[39m all_hidden_states = () \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mixer_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m     hidden_states = \u001b[43mmixer_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    767\u001b[39m         all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/transformers/models/falcon_mamba/modeling_falcon_mamba.py:564\u001b[39m, in \u001b[36mFalconMambaBlock.forward\u001b[39m\u001b[34m(self, hidden_states, cache_params, cache_position, attention_mask)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.residual_in_fp32:\n\u001b[32m    562\u001b[39m     residual = residual.to(torch.float32)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmixer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TAU/NLP/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run Full Evaluation\n",
    "print(f\"[+] Evaluating {len(train)} samples...\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "position_correct = {}\n",
    "position_total = {}\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(len(train))):\n",
    "    sample = train[i]\n",
    "    raw_input = sample[\"input\"][\"raw_input\"]\n",
    "    queried_object = sample[\"input\"].get(\"Object.0.Query\", None)\n",
    "    \n",
    "    # Get expected answer\n",
    "    forward_res = causal_model.run_forward(sample[\"input\"])\n",
    "    expected = forward_res.get(\"answer\", \"\")\n",
    "    if isinstance(expected, dict):\n",
    "        expected = max(expected, key=lambda k: expected[k])\n",
    "    \n",
    "    # Get query position\n",
    "    query_position = sample[\"input\"].get(\"metadata\", {}).get(\"src_positional_index\", -1)\n",
    "    \n",
    "    # Format and generate using model wrapper\n",
    "    prompt = model_wrapper.format_prompt(raw_input, queried_object=queried_object)\n",
    "    response = model_wrapper.generate_response(prompt)\n",
    "    \n",
    "    # Check correctness using naive checker\n",
    "    is_correct = naive_checker(response, expected)\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "    print(f\"{i=} {response=} {expected=} {is_correct=}\")\n",
    "    \n",
    "    # Track per-position accuracy\n",
    "    if query_position >= 0:\n",
    "        if query_position not in position_correct:\n",
    "            position_correct[query_position] = 0\n",
    "            position_total[query_position] = 0\n",
    "        if is_correct:\n",
    "            position_correct[query_position] += 1\n",
    "        position_total[query_position] += 1\n",
    "    \n",
    "    results.append({\n",
    "        \"expected\": expected,\n",
    "        \"response\": response,\n",
    "        \"correct\": is_correct,\n",
    "        \"position\": query_position,\n",
    "        \"queried_object\": queried_object,\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RESULTS: {model_wrapper.get_display_name()} on Binding Task (n={num_instances})\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Overall Accuracy: {correct/total:.2%} ({correct}/{total})\")\n",
    "\n",
    "if position_total:\n",
    "    print(f\"\\nPer-Position Accuracy:\")\n",
    "    for pos in sorted(position_total.keys()):\n",
    "        pos_acc = position_correct[pos] / position_total[pos]\n",
    "        print(f\"  Position {pos}: {pos_acc:.2%} ({position_correct[pos]}/{position_total[pos]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Specific Test Cells\n",
    "\n",
    "The cells below allow you to quickly test each supported model with a single sample.\n",
    "Change the model_id in the configuration cell above and run the corresponding test cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of incorrect predictions:\n",
      "  Object: disk, Expected: W, Got: ' A, B, C, D, E,', Position: 8\n",
      "  Object: glass, Expected: P, Got: ' O\n",
      "\n",
      "The ice is in Box Y,', Position: 6\n",
      "  Object: picture, Expected: S, Got: ' A, B, C, D, E,', Position: 1\n",
      "  Object: train, Expected: V, Got: ' L\n",
      "\n",
      "Which box is the plant in?', Position: 3\n",
      "  Object: clock, Expected: P, Got: ' A, B, C, D, E,', Position: 2\n",
      "  Object: medicine, Expected: R, Got: ' A, B, C, D, E,', Position: 5\n",
      "  Object: newspaper, Expected: R, Got: ' M\n",
      "\n",
      "The answer is: M\n",
      "\n",
      "', Position: 7\n",
      "  Object: pot, Expected: V, Got: ' F\n",
      "\n",
      "The bus is in Box F,', Position: 2\n",
      "  Object: bread, Expected: N, Got: ' B\n",
      "\n",
      "Which box is the bread in?', Position: 8\n",
      "  Object: bread, Expected: V, Got: ' A, B, C, D, E,', Position: 6\n",
      "\n",
      "\n",
      "  Object: suit, Expected: T, Got: ' T Box Answer: Z Box Answer: V Box', Position: 1\n",
      "  Object: mirror, Expected: D, Got: ' D\n",
      "\n",
      "Which box is the computer in?', Position: 3\n",
      "  Object: boot, Expected: W, Got: ' I, G, W, U, O,', Position: 4\n",
      "  Object: engine, Expected: Y, Got: ' Y\n",
      "\n",
      "Which box is the engine in?', Position: 9\n",
      "  Object: computer, Expected: N, Got: ' N\n",
      "\n",
      "The answer is in Box A.', Position: 9\n",
      "  Object: plate, Expected: C, Got: ' C\n",
      "\n",
      "Which box is the stone in?', Position: 5\n",
      "  Object: mirror, Expected: R, Got: ' R\n",
      "\n",
      "The answer is in Box R.', Position: 9\n",
      "  Object: train, Expected: I, Got: ' I, W, N, V, U,', Position: 4\n",
      "  Object: chemical, Expected: J, Got: ' J. Which box is the fish in? Box', Position: 0\n",
      "  Object: wire, Expected: V, Got: ' V\n",
      "\n",
      "The answer is in Box V.', Position: 9\n"
     ]
    }
   ],
   "source": [
    "# Analyze Errors\n",
    "print(\"Sample of incorrect predictions:\")\n",
    "incorrect = [r for r in results if not r[\"correct\"]]\n",
    "correct = [r for r in results if r[\"correct\"]]\n",
    "for r in incorrect[:10]:\n",
    "    print(f\"  Object: {r['queried_object']}, Expected: {r['expected']}, Got: '{r['response']}', Position: {r['position']}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "for r in correct[:10]:\n",
    "    print(f\"  Object: {r['queried_object']}, Expected: {r['expected']}, Got: '{r['response']}', Position: {r['position']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
